{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cc5c01-39dd-4a03-bed3-dff5bd71948a",
   "metadata": {},
   "source": [
    "# Performance Tuning\n",
    "\n",
    "**Spark configuration:**\n",
    "- Executor memory: `1G - 200M (reserved) * 0.6 (fraction) * 0.9 (1 - storageFraction) = 432MB`\n",
    "\n",
    "**Skewed input data:**\n",
    "- Transactions:\n",
    "  - Larges partition has 3.8M records (grouped by `instrument_id`)\n",
    "  - 3x Integer (4B), 1x Timestamp (8B)\n",
    "  - Size of larges partition: 76MB\n",
    "\n",
    "Inspect:\n",
    "- Distribution of task runtimes (straggler tasks)\n",
    "- Disk spill\n",
    "- Shuffle Read & Write\n",
    "- Shuffle Partitions (stragglers after exchange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54efc5b7-fa93-4764-9714-45194b5dd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.functions import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267daf42-12c9-4155-8751-a7a9b84b9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"performance-optimizations\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0869fb22-ad56-4a45-b95d-fa96a922f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_df = spark.read.parquet('/data/gen/transaction')\n",
    "tx_uniform_df = spark.read.parquet('/data/gen/transaction-uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec5f663-cacd-42a9-a140-afd0448b0fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|instrument_id|  count|\n",
      "+-------------+-------+\n",
      "|            2|3887000|\n",
      "|            1|3875000|\n",
      "|            3|1399000|\n",
      "|            4| 517000|\n",
      "|            5| 218000|\n",
      "|            6|  59000|\n",
      "|            7|  24000|\n",
      "|            8|  13000|\n",
      "|           10|   4000|\n",
      "|            9|   4000|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-----+\n",
      "|instrument_id|count|\n",
      "+-------------+-----+\n",
      "|         9760| 1124|\n",
      "|         8261| 1122|\n",
      "|         3496| 1121|\n",
      "|         4740| 1120|\n",
      "|         6322| 1118|\n",
      "|          326| 1109|\n",
      "|         8838| 1108|\n",
      "|         3431| 1108|\n",
      "|         6366| 1107|\n",
      "|         1598| 1105|\n",
      "|          473| 1102|\n",
      "|         4399| 1099|\n",
      "|         9975| 1098|\n",
      "|         6633| 1095|\n",
      "|         6738| 1094|\n",
      "|         9379| 1093|\n",
      "|         3662| 1093|\n",
      "|         7037| 1093|\n",
      "|          279| 1093|\n",
      "|         2896| 1093|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tx_df.groupBy('instrument_id').count().sort(col('count').desc()).show()\n",
    "tx_uniform_df.groupBy('instrument_id').count().sort(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f4efae-cc81-4afe-a654-9c08b7c2a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_df.select('instrument_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a1d442-6c2b-4860-88de-93d1b640c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|instrument_id|  count|\n",
      "+-------------+-------+\n",
      "|            2|3887000|\n",
      "|            1|3875000|\n",
      "|            3|1399000|\n",
      "|            4| 517000|\n",
      "|            5| 218000|\n",
      "|            6|  59000|\n",
      "|            7|  24000|\n",
      "|            8|  13000|\n",
      "|           10|   4000|\n",
      "|            9|   4000|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tx_df.groupBy('instrument_id').count().sort(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103b1ca-8ef1-4f60-a1fc-ac1491b9f7a9",
   "metadata": {},
   "source": [
    "## Example 1: Expensive window function\n",
    "\n",
    "### AQE enabled\n",
    "\n",
    "Observations:\n",
    "\n",
    "- Stage 1: Read parquet & exchange\n",
    "  - 8 Tasks, even record distribution, no stragglers\n",
    "  - Shuffle write 18MB / task\n",
    "- Stage 2: AQEShuffleRead, Window, Write\n",
    "  - Straggler tasks, uneven distribution of records\n",
    "  - `AQEShuffleRead` 4 shuffle partitions (coalesce)\n",
    "  - Disk spill (80MB) only in few partitions\n",
    "  - No serialization, shuffle write (write to file)\n",
    " \n",
    "### AQE disabled\n",
    "\n",
    "Observations:\n",
    "\n",
    "- Stage 1: Same execution\n",
    "- Stage 2:\n",
    "  - 200 Tasks, uneven distribution of records\n",
    "  - Disk Spill (80MB) in few partitions\n",
    "\n",
    "**Not really optimizable query except for the shuffle partitions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6659eb67-9ea8-45bc-b1b3-5ab85a962162",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a65249-037d-4bdc-be85-ee268fdedb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ts = datetime.datetime.fromisoformat('2023-01-01T00:00:00')\n",
    "window = Window \\\n",
    "    .partitionBy('instrument_id') \\\n",
    "    .orderBy(col('transaction_ts'))\n",
    "expensive_to_calculate = tx_df \\\n",
    "    .withColumn('lag_date', lag('transaction_ts', 1, default_ts).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743acabb-a963-40f2-87bb-1c434b1ae343",
   "metadata": {},
   "outputs": [],
   "source": [
    "expensive_to_calculate.write.mode(\"overwrite\").parquet('/tmp/unused-result.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35152c59-da0c-46f8-8665-217e26857214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [transaction_id#0, instrument_id#1, trader_id#2, transaction_ts#3, lag('transaction_ts, -1, 2023-01-01 00:00:00) windowspecdefinition('instrument_id, 'transaction_ts ASC NULLS FIRST, unspecifiedframe$()) AS lag_date#56]\n",
      "+- Relation [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_id: int, instrument_id: int, trader_id: int, transaction_ts: timestamp, lag_date: timestamp\n",
      "Project [transaction_id#0, instrument_id#1, trader_id#2, transaction_ts#3, lag_date#56]\n",
      "+- Project [transaction_id#0, instrument_id#1, trader_id#2, transaction_ts#3, lag_date#56, lag_date#56]\n",
      "   +- Window [lag(transaction_ts#3, -1, 2023-01-01 00:00:00) windowspecdefinition(instrument_id#1, transaction_ts#3 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_date#56], [instrument_id#1], [transaction_ts#3 ASC NULLS FIRST]\n",
      "      +- Project [transaction_id#0, instrument_id#1, trader_id#2, transaction_ts#3]\n",
      "         +- Relation [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Window [lag(transaction_ts#3, -1, 2023-01-01 00:00:00) windowspecdefinition(instrument_id#1, transaction_ts#3 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_date#56], [instrument_id#1], [transaction_ts#3 ASC NULLS FIRST]\n",
      "+- Relation [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "Window [lag(transaction_ts#3, -1, 2023-01-01 00:00:00) windowspecdefinition(instrument_id#1, transaction_ts#3 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_date#56], [instrument_id#1], [transaction_ts#3 ASC NULLS FIRST]\n",
      "+- *(2) Sort [instrument_id#1 ASC NULLS FIRST, transaction_ts#3 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(instrument_id#1, 200), ENSURE_REQUIREMENTS, [plan_id=136]\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/gen/transaction], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,instrument_id:int,trader_id:int,transaction_ts:timestamp>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expensive_to_calculate.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e9019-f109-4e4d-9eec-398f88c999cf",
   "metadata": {},
   "source": [
    "## Example 2: Skewed join\n",
    "\n",
    "- Join of two datasets\n",
    "  - Transaction: highly-skewed on `instrument_id`, 10M records\n",
    "  - Instrument: uniform distribution w.r.t `instrument_id`\n",
    "\n",
    "### AQE & BroadcastJoin disabled\n",
    "\n",
    "Observations:\n",
    "- execution time 11s\n",
    "- Stage 1: Read transactions\n",
    "  - 8 tasks, even distribution, 17MB shuffle write\n",
    "- Stage 2: Read instruments\n",
    "  - 8 tasks, even distribution, 35KB shuffle write\n",
    "  - high serialization time (why?)\n",
    "- Stage 3:\n",
    "  - Straggler tasks (few), critical path, highly uneven distribution of records\n",
    "  - Disk spill: 45MB (max) in few partitions\n",
    "\n",
    "### AQE & BroadcastJoin enabled\n",
    "\n",
    "Observations:\n",
    "- execution time 5s\n",
    "- optimized shuffle partitions (8)\n",
    "- only two stages\n",
    "  - Stage 1: Load instruments\n",
    "  - Stage 2: Load transactions, join & write to parquet\n",
    "    - even distribution of records, no stragglers\n",
    "    - 8 tasks\n",
    "    - no disk spill\n",
    "\n",
    "**Very well optimizable due to nature of input data: Instrument DataFrame is smaller than `broadcastJoinThreshold`.**\n",
    "\n",
    "Use `instrument_df.hint(\"broadcast\")` for an explicit hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f2e10cb-9494-4ca3-97a5-0d2d998b102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # default: 10485760 (10mb)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29645798-56b6-4134-a45a-262fa0cff58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_df = spark.read.parquet('/data/gen/instrument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d08c0c31-1bf6-4252-8cf1-a85b0aff7e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_df.select('instrument_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e119f2-ec36-4271-aa9b-3d2dc28e7afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [instrument_id])\n",
      ":- Relation [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] parquet\n",
      "+- Relation [instrument_id#139,instrument_type#140,registered_ts#141] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "instrument_id: int, transaction_id: int, trader_id: int, transaction_ts: timestamp, instrument_type: string, registered_ts: timestamp\n",
      "Project [instrument_id#1, transaction_id#0, trader_id#2, transaction_ts#3, instrument_type#140, registered_ts#141]\n",
      "+- Join Inner, (instrument_id#1 = instrument_id#139)\n",
      "   :- Relation [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] parquet\n",
      "   +- Relation [instrument_id#139,instrument_type#140,registered_ts#141] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [instrument_id#1, transaction_id#0, trader_id#2, transaction_ts#3, instrument_type#140, registered_ts#141]\n",
      "+- Join Inner, (instrument_id#1 = instrument_id#139)\n",
      "   :- Filter isnotnull(instrument_id#1)\n",
      "   :  +- Relation [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] parquet\n",
      "   +- Filter isnotnull(instrument_id#139)\n",
      "      +- Relation [instrument_id#139,instrument_type#140,registered_ts#141] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [instrument_id#1, transaction_id#0, trader_id#2, transaction_ts#3, instrument_type#140, registered_ts#141]\n",
      "   +- BroadcastHashJoin [instrument_id#1], [instrument_id#139], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(instrument_id#1)\n",
      "      :  +- FileScan parquet [transaction_id#0,instrument_id#1,trader_id#2,transaction_ts#3] Batched: true, DataFilters: [isnotnull(instrument_id#1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/gen/transaction], PartitionFilters: [], PushedFilters: [IsNotNull(instrument_id)], ReadSchema: struct<transaction_id:int,instrument_id:int,trader_id:int,transaction_ts:timestamp>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=681]\n",
      "         +- Filter isnotnull(instrument_id#139)\n",
      "            +- FileScan parquet [instrument_id#139,instrument_type#140,registered_ts#141] Batched: true, DataFilters: [isnotnull(instrument_id#139)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/data/gen/instrument], PartitionFilters: [], PushedFilters: [IsNotNull(instrument_id)], ReadSchema: struct<instrument_id:int,instrument_type:string,registered_ts:timestamp>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tx_df.join(instrument_df, 'instrument_id') \\\n",
    "    .explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08cdf5ea-eca9-46c7-91cb-5c9d60cf3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_df.join(instrument_df, 'instrument_id') \\\n",
    "    .write.mode('overwrite').parquet('/tmp/unused-result.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db904c4-03a9-4d20-85c6-257a5385f4c1",
   "metadata": {},
   "source": [
    "## Salting implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dbf27e3-2c7c-452a-a4ce-fe00b79f5e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|instrument_id_salted| count|\n",
      "+--------------------+------+\n",
      "|                 2_2|415000|\n",
      "|                 1_1|406000|\n",
      "|                 1_6|403000|\n",
      "|                 1_4|401000|\n",
      "|                 1_9|400000|\n",
      "|                 1_7|399000|\n",
      "|                 2_6|396000|\n",
      "|                 1_3|391000|\n",
      "|                 2_4|391000|\n",
      "|                 2_8|391000|\n",
      "|                 2_3|387000|\n",
      "|                 2_7|387000|\n",
      "|                 2_0|386000|\n",
      "|                 2_9|382000|\n",
      "|                 2_5|380000|\n",
      "|                 1_5|376000|\n",
      "|                 2_1|372000|\n",
      "|                 1_2|369000|\n",
      "|                 1_8|368000|\n",
      "|                 1_0|362000|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tx_df.withColumn('instrument_id_salted', concat(col('instrument_id'), lit('_'), col('transaction_id') % 10)).groupBy('instrument_id_salted').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03baee09-07de-4d56-989b-71ec335a26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------------+---+--------------------+\n",
      "|instrument_id|instrument_type|      registered_ts| id|instrument_id_salted|\n",
      "+-------------+---------------+-------------------+---+--------------------+\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  0|              1250_0|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  1|              1250_1|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  2|              1250_2|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  3|              1250_3|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  4|              1250_4|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  5|              1250_5|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  6|              1250_6|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  7|              1250_7|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  8|              1250_8|\n",
      "|         1250|            ETF|2017-10-23 23:00:00|  9|              1250_9|\n",
      "+-------------+---------------+-------------------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salting_keys = spark.range(0, 10)\n",
    "instrument_df.join(salting_keys, how='cross').withColumn('instrument_id_salted', concat(col('instrument_id'), lit('_'), col('id'))).filter(col('instrument_id') == 1250).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc176f2-a586-40ca-8b2d-5b26c879d4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
